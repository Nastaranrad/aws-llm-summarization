
# Dialogue Summarization with LLMs (FLAN-T5 + LoRA) â€” AWS Portfolio

This repository demonstrates **text summarization with Large Language Models (LLMs)** using the
**DialogSum** dataset and a **FLANâ€‘T5** model.

> Highlights
> - Zeroâ€‘shot baseline with `google/flan-t5-base`
> - Full fineâ€‘tuning (Trainer) *and* PEFT/LoRA fineâ€‘tuning path
> - ROUGE evaluation vs. ground truth
> - Readyâ€‘toâ€‘use CLI for inference
> - AWSâ€‘friendly: works in a SageMaker Notebook / Studio or locally

---

## ðŸ§± Project Structure

```
aws-llm-summarization-portfolio/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ infer.py                  # Run summaries from a file or prompt
â”œâ”€â”€ sample_dialog.txt         # Example input
â””â”€â”€ notebooks/                # (Optional) put course lab notebook here
```

---

## ðŸ“¦ Setup

```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

> If using **SageMaker**, you can install requirements directly in the notebook kernel.

---

## ðŸš€ Quick Start (Inference)

Run a quick summary using the base FLANâ€‘T5 model:

```bash
python infer.py --text "Alice and Bob discussed travel plans. They agreed to meet Friday and book tickets."
```

Or summarize a file with multiple lines (one example per line):

```bash
python infer.py --file sample_dialog.txt
```

> To use a **fineâ€‘tuned** checkpoint, pass `--checkpoint /path/to/checkpoint`.

---

## ðŸ“š Dataset

- **DialogSum** (`knkarthick/dialogsum`) from Hugging Face Datasets.

---

## ðŸ”§ Baseline & Training (What the lab does)

1. **Zeroâ€‘shot baseline** with `google/flan-t5-base`
   - Prepend an instruction like: `"Summarize the following dialogue:"`
   - Generate with beam search or sampling (temperature).

2. **Full Fineâ€‘Tuning** (Hugging Face `Trainer`)
   - Tokenize inputs (`dialogue`) and labels (`summary`)
   - Train with a small number of epochs to keep costs low
   - Save checkpoint locally or to S3 (if in AWS)

3. **PEFT/LoRA Fineâ€‘Tuning**
   - Wrap the base model with LoRA adapters
   - Train adapters only (much faster/cheaper)
   - Load adapters for inference

4. **Evaluation with ROUGE**
   - Compute ROUGEâ€‘1/2/L on test samples
   - Compare *original model* vs *fineâ€‘tuned* vs *LoRA*

---

## ðŸ§ª Reproduce (locally or SageMaker)

**Zeroâ€‘shot & evaluation** can be reproduced fully locally. For fineâ€‘tuning, prefer a GPU (or SageMaker).

Highâ€‘level steps:
1. Load dataset and split
2. Tokenize with the FLANâ€‘T5 tokenizer (`t5` family uses `input_ids`/`labels`)
3. Train either:
   - Full fineâ€‘tuning with `Trainer`
   - PEFT/LoRA with `peft.get_peft_model`
4. Save checkpoint to `./checkpoints/...` (or S3 in SageMaker)
5. Evaluate ROUGE with `evaluate`

---

